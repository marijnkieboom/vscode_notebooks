{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "from utils.Vocabulary import Vocabulary\n",
    "\n",
    "min_freq = 75\n",
    "eng_vocab_file = \"../data-sets/vocab_eng.json\"\n",
    "nld_vocab_file = \"../data-sets/vocab_nld.json\"\n",
    "\n",
    "with open(eng_vocab_file, mode=\"r\") as f:\n",
    "    eng_dict = json.load(f, object_pairs_hook=OrderedDict)\n",
    "\n",
    "with open(nld_vocab_file, mode=\"r\") as f:\n",
    "    nld_dict = json.load(f, object_pairs_hook=OrderedDict)\n",
    "\n",
    "eng_vocab = Vocabulary(eng_dict, min_freq=min_freq)\n",
    "nld_vocab = Vocabulary(nld_dict, min_freq=min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data-sets/filtered_and_tokenized_sentences(1).csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred: unable to open shared memory object </torch_19855_1496070290_12549> in read-write mode: Too many open files in system (23)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mresult\u001b[49m  \u001b[38;5;66;03m# Release shared memory objects\u001b[39;00m\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m future  \u001b[38;5;66;03m# Release the future object\u001b[39;00m\n\u001b[1;32m     73\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "import concurrent\n",
    "\n",
    "max_workers=4\n",
    "chunksize=10000\n",
    "columns = [\"eng_tokens\", \"nld_tokens\"]\n",
    "dtype = { \"eng_tokens\": \"str\", \"nld_tokens\": \"str\" }\n",
    "\n",
    "def init(input_vocabulary, target_vocabulary):\n",
    "    global input_vocab, target_vocab\n",
    "    input_vocab = input_vocabulary\n",
    "    target_vocab = target_vocabulary\n",
    "\n",
    "def process_chunk(df):\n",
    "    # return [(torch.zeros(20, dtype=torch.int), torch.zeros(20, dtype=torch.int))]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        result.append((torch.zeros(20, dtype=torch.int), torch.zeros(20, dtype=torch.int)))\n",
    "\n",
    "        # eng_idx = [input_vocab.token_to_index_func(token) for token in row[\"eng_tokens\"].split()]\n",
    "        # nld_idx = [target_vocab.token_to_index_func(token) for token in row[\"nld_tokens\"].split()]\n",
    "\n",
    "        # eng_idx = pad_or_truncate(eng_idx, input_vocab.token_to_index_func(\"<PAD>\"))\n",
    "        # nld_idx = pad_or_truncate(nld_idx, target_vocab.token_to_index_func(\"<PAD>\"))\n",
    "\n",
    "        # result.append((torch.tensor(eng_idx, dtype=torch.int), torch.tensor(nld_idx, dtype=torch.int)))\n",
    "\n",
    "    return result\n",
    "\n",
    "def pad_or_truncate(tokens, pad_idx):\n",
    "    output_size = 20\n",
    "\n",
    "    if (len(tokens) == output_size):\n",
    "        return tokens\n",
    "\n",
    "    if len(tokens) > output_size:\n",
    "        return tokens[:output_size]\n",
    "\n",
    "    return tokens + [pad_idx] * (output_size - len(tokens))\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    encoding=\"utf-8\",\n",
    "    header=None,\n",
    "    names=columns,\n",
    "    dtype=dtype,\n",
    "    nrows=1000000\n",
    ")\n",
    "\n",
    "chunks = [df[i:i+chunksize] for i in range(0, df.shape[0], chunksize)]\n",
    "\n",
    "results = []\n",
    "with concurrent.futures.ProcessPoolExecutor(\n",
    "    max_workers=max_workers,\n",
    "    initializer=init,\n",
    "    initargs=(eng_vocab, nld_vocab)\n",
    ") as executor:\n",
    "    futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "        finally:\n",
    "            del result  # Release shared memory objects\n",
    "            del future  # Release the future object\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Total time:\", t1-t0)\n",
    "print()\n",
    "print('Size:', len(results))\n",
    "print()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 37.085352182388306\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "output_size = 20\n",
    "\n",
    "max_workers=None\n",
    "chunksize=100000\n",
    "columns = [\"eng_tokens\", \"nld_tokens\"]\n",
    "dtype = { \"eng_tokens\": \"str\", \"nld_tokens\": \"str\" }\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "with pd.read_csv(\n",
    "        file_path,\n",
    "        encoding=\"utf-8\",\n",
    "        chunksize=chunksize,\n",
    "        header=None,\n",
    "        names=columns,\n",
    "        dtype=dtype,\n",
    "        nrows=1000000\n",
    "    ) as reader:\n",
    "    for chunk in reader:\n",
    "        for _, row in chunk.iterrows():\n",
    "            eng_idx = [eng_vocab.token_to_index_func(token) for token in row[\"eng_tokens\"].split()]\n",
    "            nld_idx = [nld_vocab.token_to_index_func(token) for token in row[\"nld_tokens\"].split()]\n",
    "\n",
    "            eng_idx = pad_or_truncate(eng_idx, eng_vocab.token_to_index_func(\"<PAD>\"))\n",
    "            nld_idx = pad_or_truncate(nld_idx, nld_vocab.token_to_index_func(\"<PAD>\"))\n",
    "\n",
    "            data.append((torch.tensor(eng_idx, dtype=torch.int), torch.tensor(nld_idx, dtype=torch.int)))\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Total time:\", t1-t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
